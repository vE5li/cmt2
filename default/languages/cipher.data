#comment_tokenizer {
    #note {
        "TODO "         todo
        "FIX "          fix
        "WIP "          wip
    }
    #block_comment  [ "@@" "@@" ]
    #line_comment   '@'
}

#number_tokenizer {
    #format {
        #none {
            #none           decimal
            'D'             decimal
            'B'             binary
            'Z'             duodecimal
            'X'             hexadecimal
        }
    }
    #system {
        binary          [ '0' '1'                                                         ]
        decimal         [ '0' '1' '2' '3' '4' '5' '6' '7' '8' '9'                         ]
        duodecimal      [ '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'X' 'E'                 ]
        hexadecimal     [ '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' ]
    }
    #float          '.'
}

#character_tokenizer {
    #replace {
        "null"          '\0'
        "backslash"     '\\'
        "backspace"     '\b'
        "character"     '\''
        "string"        '\"'
        "newline"       '\n'
        "tab"           '\t'
        "return"        '\r'
        "escape"        '\e'
        "open"          '{'
        "close"         '}'
    }
    #delimiter      [ '\'' '\'' ]
}

#string_tokenizer {
    #replace {
        "{null}"        '\0'
        "{backslash}"   '\\'
        "{backspace}"   '\b'
        "{character}"   '\''
        "{string}"      '\"'
        "{newline}"     '\n'
        "{tab}"         '\t'
        "{return}"      '\r'
        "{escape}"      '\e'
        "{open}"        '{'
        "{close}"       '}'
    }
    #delimiter      [ '\"' '\"' ]
}

#operator_tokenizer {
    #translate {
        "=="            identical
        "!="            differ
        ":="            assign
        "+="            add_assign
        "-="            subtract_assign
        "/="            divide_assign
        "*="            multiply_assign
        "&="            and_assign
        "|="            or_assign
        "^="            xor_assign
        '>'             bigger
        '<'             smaller
        '+'             add
        '-'             subtract
        '/'             divide
        '*'             multiply
        '|'             or
        '&'             and
        '!'             invert
        '~'             concatinate
        "^^"            logical_xor
        "||"            logical_or
        "&&"            logical_and
        '^'             xor
        '%'             modulus
        "::"            contains
        "->"            case
        '?'             macro
        '{'             open_curly
        '}'             close_curly
        '('             open_round
        ')'             close_round
        '['             open_square
        ']'             close_square
        '#'             selector
        '$'             image
        ':'             colon
        ','             comma
        '.'             dot
    }
    #ignored        [ '\r' '\t' '\n' ' ' ]
    #invalid        [ '\\' ';' '=' ]
}

#keyword_tokenizer {
    #translate {
        true            true
        false           false
        public          public
        label           label
        loop            loop
        while           while
        iterate         iterate
        collect         collect
        if              if
        module          module
        namespace       namespace
        import          import
        export          export
        case            case
        alias           alias
        macro           macro
        match           match
        trait           trait
        any             any
        implement       implement
        derive          derive
        type            type
        not             not
        as              as
        for             for
        return          return
        pass            pass
        terminate       terminate
        continue        continue
        break           break
        final           final
        mutable         mutable
        implied         implied
        self            self
        super           super
        root            root
        base            base
    }
}

#identifier_tokenizer {
    #type_prefix     [ _ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ]
    #prefix          [ _ a b c d e f g h i j k l m n o p q r s t u v w x y z ]
}
