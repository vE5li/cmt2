#comment_tokenizer {
    #block_comment  [ "/*" "*/" ]
    #line_comment   "//"
}

#number_tokenizer {
    #format {
        #none {
            #none           decimal
        }
        "0x" {
            #none           hexadecimal
        }
        "0b" {
            #none           binary
        }
    }
    #system {
        binary          [ '0' '1'                                                         ]
        decimal         [ '0' '1' '2' '3' '4' '5' '6' '7' '8' '9'                         ]
        hexadecimal     [ '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' ]
    }
    #float          '.'
}

#character_tokenizer {
    #replace {
        "\\0"           '\0'
        "\\\\"          '\\'
        "\\b"           '\b'
        "\\\'"          '\''
        "\\\""          '\"'
        "\\n"           '\n'
        "\\t"           '\t'
        "\\r"           '\r'
        "\\e"           '\e'
    }
    #delimiter      [ '\'' '\'' ]
}

#string_tokenizer {
    #replace {
        "\\0"           '\0'
        "\\\\"          '\\'
        "\\b"           '\b'
        "\\\'"          '\''
        "\\\""          '\"'
        "\\n"           '\n'
        "\\t"           '\t'
        "\\r"           '\r'
        "\\e"           '\e'
    }
    #delimiter      [ '\"' '\"' ]
}

#operator_tokenizer {
    #translate {
        "..."           inclusive_range
        "..="           inclusive_range
        ".."            exclusive_range
        "=="            identical
        "!="            differ
        ">="            grater_equal
        "<="            smaller_equal
        "+="            add_assign
        "-="            subtract_assign
        "/="            divide_assign
        "*="            multiply_assign
        "&="            and_assign
        "|="            or_assign
        "^="            xor_assign
        "%="            modulo_assign
        '='             assign
        '>'             bigger
        '<'             smaller
        '+'             add
        '-'             subtract
        '/'             divide
        '*'             multiply
        '&'             and
        '|'             or
        '!'             not
        '%'             modulo
        '~'             concatinate
        "^^"            logical_xor
        "||"            logical_or
        "&&"            logical_and
        '^'             xor
        "::"            contains
        "->"            return
        '?'             error
        '{'             open_curly
        '}'             close_curly
        '('             open_round
        ')'             close_round
        '['             open_square
        ']'             close_square
        '#'             selector
        '$'             image
        ':'             colon
        ','             comma
        '.'             member
        '@'             pattern
        ';'             semicolon
    }
    #ignored        [ '\r' '\t' '\n' ' ' ]
    #invalid        [ '\\' ]
}

#keyword_tokenizer {
    #translate {
        as              as
        break           break
        const           const
        continue        continue
        crate           crate
        else            else
        enum            enum
        extern          extern
        false           false
        fn              fn
        for             for
        if              if
        impl            impl
        in              in
        let             let
        loop            loop
        match           match
        mod             mod
        move            move
        mut             mut
        pub             pub
        ref             ref
        return          return
        self            self
        static          static
        struct          struct
        super           super
        true            true
        type            type
        unsafe          unsafe
        use             use
        where           where
        while           while
        async           async
        await           await
        dyn             dyn
    }
}

#identifier_tokenizer {
    #type_prefix     [ _ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z ]
    #prefix          [ _ a b c d e f g h i j k l m n o p q r s t u v w x y z ]
}
